{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdsZ0jj9n6R7",
        "outputId": "618b45c9-c42b-4a98-ebf3-638c7a6a736f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQwUzA-Tf6Q-"
      },
      "source": [
        "## Required Libraries\n",
        "\n",
        "Before running this notebook, make sure you have installed the following libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSuoZCH3oDhH",
        "outputId": "c7553f2b-4fe0-42f3-8197-2019a814d877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (1.6.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (3.0.2)\n",
            "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-metric-learning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pytorch-metric-learning-2.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-metric-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TU0LtNdoDdj",
        "outputId": "bac51477-c879-4ca6-d9ba-d60239cca5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.16.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_scatter, pyg_lib, torch_sparse\n",
            "Successfully installed pyg_lib-0.4.0+pt26cu124 torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MIAnwWI8oDZ9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import math\n",
        "from math import log\n",
        "\n",
        "file_path = 'edge_probability_career'\n",
        "\n",
        "\n",
        "eigen_vector='eigenvector.pkl'\n",
        "\n",
        "outdegree='outdegree.pkl'\n",
        "\n",
        "node_to_index_mapping='node_to_index_mapping.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    edge_probability_career = pickle.load(file)\n",
        "\n",
        "with open(eigen_vector, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    eigenvector = pickle.load(file)\n",
        "\n",
        "\n",
        "with open(outdegree, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    outdegree = pickle.load(file)\n",
        "\n",
        "\n",
        "with open(node_to_index_mapping, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    node_to_index_mapping = pickle.load(file)\n",
        "\n",
        "def outdegree_graph1():\n",
        "    list_outdegree=[]\n",
        "    for key in outdegree:\n",
        "        list_outdegree.append(outdegree[key])\n",
        "    max_outdegree=max(list_outdegree)\n",
        "    min_outdegree=min(list_outdegree)\n",
        "\n",
        "    dict1_out_degree={}\n",
        "    list_follower=set()\n",
        "    for key in edge_probability_career:\n",
        "        dict1={}\n",
        "        for follower in edge_probability_career[key]:\n",
        "            list1=[]\n",
        "            for edge in edge_probability_career[key][follower]:\n",
        "                source_node_degree=outdegree[key]\n",
        "                tail_node_degree=outdegree[follower]\n",
        "                probability=float(source_node_degree+tail_node_degree)/2\n",
        "                tag=edge[0]\n",
        "                list1.append((tag,probability))\n",
        "            dict1[follower]=list1\n",
        "        dict1_out_degree[key]=dict1\n",
        "    return dict1_out_degree\n",
        "\n",
        "\n",
        "# augmenting with eigenvector\n",
        "\n",
        "def eigenvector1():\n",
        "    list_eigenvector=[]\n",
        "    for key in eigenvector:\n",
        "        list_eigenvector.append(eigenvector[key])\n",
        "\n",
        "    #indegree_average=sum(list_indegree)/len(list_indegree)\n",
        "    max_eigenvector=max(list_eigenvector)\n",
        "    min_eigenvector=min(list_eigenvector)\n",
        "    #print(\"\")\n",
        "\n",
        "    dict1_eigenvector={}  # this file will be like edge_probablity career having probaboility as outdegree\n",
        "\n",
        "\n",
        "    for key in edge_probability_career:\n",
        "        dict1={}\n",
        "        for follower in edge_probability_career[key]:\n",
        "            list1=[]\n",
        "            for edge in edge_probability_career[key][follower]:\n",
        "                source_node_degree=eigenvector[key]\n",
        "                tail_node_degree=eigenvector[follower]\n",
        "                probability=float(source_node_degree+tail_node_degree)/2\n",
        "                #probability=float(eigenvector[follower]-min_eigenvector)/(max_eigenvector-min_eigenvector)\n",
        "                #probability=0.4\n",
        "                #print(\"\")\n",
        "                tag=edge[0]\n",
        "                list1.append((tag,probability))\n",
        "            dict1[follower]=list1\n",
        "        dict1_eigenvector[key]=dict1\n",
        "    return dict1_eigenvector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX04hDmYhq0g"
      },
      "source": [
        "## Generates augmented graph data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DcKYO5KQoDWt"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import math\n",
        "from math import log\n",
        "import random\n",
        "\n",
        "file_path = 'edge_probability_career'\n",
        "\n",
        "\n",
        "eigen_vector='eigenvector.pkl'\n",
        "\n",
        "pagerank_file='page_rank.pkl'\n",
        "with open(pagerank_file, 'rb') as file:\n",
        "    pagerank = pickle.load(file)\n",
        "\n",
        "outdegree_file = 'outdegree.pkl'\n",
        "with open(outdegree_file, 'rb') as file:\n",
        "    outdegree = pickle.load(file)\n",
        "\n",
        "\n",
        "indegree_file = 'indegree.pkl'\n",
        "with open(indegree_file, 'rb') as file:\n",
        "    indegree = pickle.load(file)\n",
        "\n",
        "node_to_index_mapping='node_to_index_mapping.pkl'\n",
        "import numpy as np\n",
        "\n",
        "node_features=\"node_features.pkl\"\n",
        "\n",
        "\n",
        "with open('edge_features_v2', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    edge_features = pickle.load(file)\n",
        "\n",
        "#print(random_array)\n",
        "with open('edge_probability_career', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    edge_probability_career = pickle.load(file)\n",
        "\n",
        "\n",
        "with open('node_features.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    node_features = pickle.load(file)\n",
        "\n",
        "\n",
        "set_all_nodes = []\n",
        "\n",
        "# Add keys first\n",
        "for key in edge_probability_career:\n",
        "    if key not in set_all_nodes:\n",
        "        set_all_nodes.append(key)\n",
        "\n",
        "# Then add followers\n",
        "for key in edge_probability_career:\n",
        "    for follower in edge_probability_career[key]:\n",
        "        if follower not in set_all_nodes:\n",
        "            set_all_nodes.append(follower)\n",
        "\n",
        "\n",
        "dict_mapping={} # key node id value index id\n",
        "index=0\n",
        "\n",
        "for node in set_all_nodes:\n",
        "    dict_mapping[node]=index\n",
        "    index=index+1\n",
        "\n",
        "list_of_nodes = list(set_all_nodes)\n",
        "\n",
        "list_node_features=[] # 0 index node feature will be first\n",
        "\n",
        "for key in dict_mapping:\n",
        "    try:\n",
        "        list_node_features.append(node_features[key])\n",
        "    except:\n",
        "        # list_node_features.append(np.random.normal(loc=0, scale=1, size=100).astype(np.float32))\n",
        "        list_node_features.append(np.full((100,), 0.5, dtype=np.float32))\n",
        "\n",
        "# Load fairness score dictionary\n",
        "with open('node_fairness_scores.pkl', 'rb') as f:\n",
        "    fairness_score_dict = pickle.load(f)\n",
        "\n",
        "# Build fairness_of_nodes list aligned with dict_mapping order\n",
        "fairness_of_nodes = []\n",
        "for node in dict_mapping:\n",
        "    fairness_val = fairness_score_dict[node]  # fallback to random if missing\n",
        "    fairness_of_nodes.append(fairness_val)\n",
        "\n",
        "\n",
        "outdegree_graph=outdegree_graph1()\n",
        "\n",
        "\n",
        "eigenvector_graph=eigenvector1()\n",
        "\n",
        "\n",
        "# random_augentation_number=2\n",
        "\n",
        "\n",
        "\n",
        "def func_graph_augmentation(random_augentation_number):\n",
        "#random_augentation_number=2\n",
        "\n",
        "    if(random_augentation_number==1):\n",
        "        dict_outdegree_edge_removed={}\n",
        "        for key in outdegree_graph:\n",
        "            dict1={}\n",
        "            for follower in outdegree_graph[key]:\n",
        "                list1=[]\n",
        "                for edge in outdegree_graph[key][follower]:\n",
        "                    if(edge[1]*0.4>random.uniform(0, 1)):  # 0.5\n",
        "                    # if(edge[1]*0.4>.2):\n",
        "                        tag=edge[0]\n",
        "                        probability=edge[1]\n",
        "                        list1.append((tag,probability))\n",
        "                if(len(list1)>0):\n",
        "                    dict1[follower]=list1\n",
        "            if(follower in dict1):\n",
        "                dict_outdegree_edge_removed[key]=dict1\n",
        "        data_graph=graph_return(dict_outdegree_edge_removed)\n",
        "\n",
        "        return  data_graph\n",
        "\n",
        "\n",
        "    if(random_augentation_number==2):\n",
        "\n",
        "        dict_inf_probablity_removed={}\n",
        "        for key in edge_probability_career:\n",
        "            dict1={}\n",
        "            for follower in edge_probability_career[key]:\n",
        "                list1=[]\n",
        "                for edge in edge_probability_career[key][follower]:\n",
        "                    if(edge[1]*0.8>random.uniform(0, 1)):   # 0.4\n",
        "                    # if(edge[1]*0.8>.2):\n",
        "                        tag=edge[0]\n",
        "                        probability=edge[1]\n",
        "                        list1.append((tag,probability))\n",
        "                if(len(list1)>0):\n",
        "                    dict1[follower]=list1\n",
        "            if(follower in dict1):\n",
        "                dict_inf_probablity_removed[key]=dict1\n",
        "        data_graph=graph_return(dict_inf_probablity_removed)\n",
        "\n",
        "        return  data_graph\n",
        "# create the graph data here######################################################################################\n",
        "\n",
        "def graph_return(graph):\n",
        "    list_source=[]\n",
        "    list_target=[]\n",
        "    edge_features_view=[]\n",
        "    for key in graph:\n",
        "        for follower in graph[key]:\n",
        "            list_source.append(dict_mapping[key])\n",
        "            list_target.append(dict_mapping[follower])\n",
        "            edge_features_view.append(edge_features[key][follower])\n",
        "    edge_index = torch.tensor([list_source, list_target], dtype=torch.long)\n",
        "    node_features = torch.tensor(np.array(list_node_features), dtype=torch.float)\n",
        "    result_tensor = torch.stack(edge_features_view)\n",
        "    result_tensor = torch.squeeze(result_tensor, dim=1)          # this result tensor is the edge features\n",
        "    fairness_tensor = torch.tensor(fairness_of_nodes, dtype=torch.float64)  # Shape: [num_nodes]\n",
        "    data_graph = Data(\n",
        "        x=node_features,\n",
        "        edge_index=edge_index,\n",
        "        edge_attr=result_tensor,\n",
        "        fairness=fairness_tensor,\n",
        "        node_id=torch.tensor(list(dict_mapping.keys()), dtype=torch.long)\n",
        "    )\n",
        "    return data_graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9wmpqrjgUKE"
      },
      "source": [
        "\n",
        "## **PNAConv-based GNN model** for 500 epochs and stores the learned node embeddings in `embeddings_fairness_mul.pkl`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OzkyZJfoDQT",
        "outputId": "892364e5-9eb6-4512-dd85-f5dc2706f5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svmem(total=13608370176, available=11800911872, percent=13.3, used=1471893504, free=621801472, active=756424704, inactive=11728519168, buffers=127971328, cached=11386703872, shared=2285568, slab=361078784)\n",
            "2.6.0+cu124\n",
            "2.6.1\n",
            "epoch 1, loss: 5.882423019409179\n",
            "epoch 2, loss: 5.5083636283874515\n",
            "epoch 3, loss: 5.360521841049194\n",
            "epoch 4, loss: 5.1496809959411625\n",
            "epoch 5, loss: 4.899170064926148\n",
            "epoch 6, loss: 4.6504627704620365\n",
            "epoch 7, loss: 4.6272365093231205\n",
            "epoch 8, loss: 4.531843566894532\n",
            "epoch 9, loss: 4.487884187698365\n",
            "epoch 10, loss: 4.374628376960755\n",
            "epoch 11, loss: 4.225434589385986\n",
            "epoch 12, loss: 4.088127851486206\n",
            "epoch 13, loss: 4.0844806432724\n",
            "epoch 14, loss: 3.9774004220962524\n",
            "epoch 15, loss: 3.726543927192688\n",
            "epoch 16, loss: 3.6470701932907104\n",
            "epoch 17, loss: 3.5548145055770872\n",
            "epoch 18, loss: 3.413642644882202\n",
            "epoch 19, loss: 3.3182846546173095\n",
            "epoch 20, loss: 3.212191343307495\n",
            "epoch 21, loss: 3.1815932989120483\n",
            "epoch 22, loss: 3.097190523147583\n",
            "epoch 23, loss: 3.001883888244629\n",
            "epoch 24, loss: 2.9528168201446534\n",
            "epoch 25, loss: 2.9401316404342652\n",
            "epoch 26, loss: 2.9542484283447266\n",
            "epoch 27, loss: 2.9678253650665285\n",
            "epoch 28, loss: 2.8540592193603516\n",
            "epoch 29, loss: 2.8265619039535523\n",
            "epoch 30, loss: 2.8051909446716308\n",
            "epoch 31, loss: 2.795500135421753\n",
            "epoch 32, loss: 2.7957441806793213\n",
            "epoch 33, loss: 2.7255117416381838\n",
            "epoch 34, loss: 2.722468543052673\n",
            "epoch 35, loss: 2.68540735244751\n",
            "epoch 36, loss: 2.7156432628631593\n",
            "epoch 37, loss: 2.657314085960388\n",
            "epoch 38, loss: 2.6314069032669067\n",
            "epoch 39, loss: 2.5921948671340944\n",
            "epoch 40, loss: 2.5683264255523683\n",
            "epoch 41, loss: 2.563823103904724\n",
            "epoch 42, loss: 2.5329397439956667\n",
            "epoch 43, loss: 2.548686647415161\n",
            "epoch 44, loss: 2.5210409641265867\n",
            "epoch 45, loss: 2.528221535682678\n",
            "epoch 46, loss: 2.5250950336456297\n",
            "epoch 47, loss: 2.546057200431824\n",
            "epoch 48, loss: 2.5256605625152586\n",
            "epoch 49, loss: 2.519775724411011\n",
            "epoch 50, loss: 2.516702151298523\n",
            "epoch 51, loss: 2.480746364593506\n",
            "epoch 52, loss: 2.4709628582000733\n",
            "epoch 53, loss: 2.4569005966186523\n",
            "epoch 54, loss: 2.4606107473373413\n",
            "epoch 55, loss: 2.4426765203475953\n",
            "epoch 56, loss: 2.4517470955848695\n",
            "epoch 57, loss: 2.445636546611786\n",
            "epoch 58, loss: 2.452645492553711\n",
            "epoch 59, loss: 2.445371651649475\n",
            "epoch 60, loss: 2.444436049461365\n",
            "epoch 61, loss: 2.4355029821395875\n",
            "epoch 62, loss: 2.4399081826210023\n",
            "epoch 63, loss: 2.4410543203353883\n",
            "epoch 64, loss: 2.45577929019928\n",
            "epoch 65, loss: 2.4654144763946535\n",
            "epoch 66, loss: 2.47110652923584\n",
            "epoch 67, loss: 2.4634737610816955\n",
            "epoch 68, loss: 2.4734830498695373\n",
            "epoch 69, loss: 2.450263702869415\n",
            "epoch 70, loss: 2.4374558210372923\n",
            "epoch 71, loss: 2.418129897117615\n",
            "epoch 72, loss: 2.416791534423828\n",
            "epoch 73, loss: 2.431150126457214\n",
            "epoch 74, loss: 2.430722713470459\n",
            "epoch 75, loss: 2.4271644711494447\n",
            "epoch 76, loss: 2.417305624485016\n",
            "epoch 77, loss: 2.4057831525802613\n",
            "epoch 78, loss: 2.398702609539032\n",
            "epoch 79, loss: 2.411799943447113\n",
            "epoch 80, loss: 2.431688737869263\n",
            "epoch 81, loss: 2.4314793229103087\n",
            "epoch 82, loss: 2.4278018951416014\n",
            "epoch 83, loss: 2.4197171688079835\n",
            "epoch 84, loss: 2.426191782951355\n",
            "epoch 85, loss: 2.435963523387909\n",
            "epoch 86, loss: 2.419728147983551\n",
            "epoch 87, loss: 2.4349284887313845\n",
            "epoch 88, loss: 2.4182443618774414\n",
            "epoch 89, loss: 2.41337274312973\n",
            "epoch 90, loss: 2.425878882408142\n",
            "epoch 91, loss: 2.424366664886475\n",
            "epoch 92, loss: 2.412155556678772\n",
            "epoch 93, loss: 2.3959994435310366\n",
            "epoch 94, loss: 2.390039157867432\n",
            "epoch 95, loss: 2.393492031097412\n",
            "epoch 96, loss: 2.391417181491852\n",
            "epoch 97, loss: 2.3919827938079834\n",
            "epoch 98, loss: 2.3934025049209593\n",
            "epoch 99, loss: 2.3937039136886598\n",
            "epoch 100, loss: 2.4113744497299194\n",
            "epoch 101, loss: 2.413559818267822\n",
            "epoch 102, loss: 2.414653778076172\n",
            "epoch 103, loss: 2.4122833251953124\n",
            "epoch 104, loss: 2.4209224581718445\n",
            "epoch 105, loss: 2.41964989900589\n",
            "epoch 106, loss: 2.4142453551292418\n",
            "epoch 107, loss: 2.398385727405548\n",
            "epoch 108, loss: 2.3967291951179504\n",
            "epoch 109, loss: 2.395326256752014\n",
            "epoch 110, loss: 2.3886345863342284\n",
            "epoch 111, loss: 2.3849493503570556\n",
            "epoch 112, loss: 2.3856090784072874\n",
            "epoch 113, loss: 2.3876134872436525\n",
            "epoch 114, loss: 2.3969638109207154\n",
            "epoch 115, loss: 2.3757345199584963\n",
            "epoch 116, loss: 2.374718153476715\n",
            "epoch 117, loss: 2.3799410820007325\n",
            "epoch 118, loss: 2.380593943595886\n",
            "epoch 119, loss: 2.385726976394653\n",
            "epoch 120, loss: 2.397919404506683\n",
            "epoch 121, loss: 2.396085238456726\n",
            "epoch 122, loss: 2.374789094924927\n",
            "epoch 123, loss: 2.370710074901581\n",
            "epoch 124, loss: 2.367773652076721\n",
            "epoch 125, loss: 2.3893799066543577\n",
            "epoch 126, loss: 2.3928141236305236\n",
            "epoch 127, loss: 2.3919278979301453\n",
            "epoch 128, loss: 2.379650318622589\n",
            "epoch 129, loss: 2.3609960436820985\n",
            "epoch 130, loss: 2.3618861079216003\n",
            "epoch 131, loss: 2.360629415512085\n",
            "epoch 132, loss: 2.3619407176971436\n",
            "epoch 133, loss: 2.3684902787208557\n",
            "epoch 134, loss: 2.3758633375167846\n",
            "epoch 135, loss: 2.3660754323005677\n",
            "epoch 136, loss: 2.3717060685157776\n",
            "epoch 137, loss: 2.363987410068512\n",
            "epoch 138, loss: 2.364391827583313\n",
            "epoch 139, loss: 2.3674555659294128\n",
            "epoch 140, loss: 2.3840776324272155\n",
            "epoch 141, loss: 2.3719514131546022\n",
            "epoch 142, loss: 2.3750521183013915\n",
            "epoch 143, loss: 2.3688525199890136\n",
            "epoch 144, loss: 2.364919900894165\n",
            "epoch 145, loss: 2.366362988948822\n",
            "epoch 146, loss: 2.3778276801109315\n",
            "epoch 147, loss: 2.3614915370941163\n",
            "epoch 148, loss: 2.357604146003723\n",
            "epoch 149, loss: 2.357773780822754\n",
            "epoch 150, loss: 2.35729923248291\n",
            "epoch 151, loss: 2.374616491794586\n",
            "epoch 152, loss: 2.3598451614379883\n",
            "epoch 153, loss: 2.36400306224823\n",
            "epoch 154, loss: 2.3603574514389036\n",
            "epoch 155, loss: 2.361070215702057\n",
            "epoch 156, loss: 2.3768006205558776\n",
            "epoch 157, loss: 2.357250452041626\n",
            "epoch 158, loss: 2.3639851212501526\n",
            "epoch 159, loss: 2.358454871177673\n",
            "epoch 160, loss: 2.3716397523880004\n",
            "epoch 161, loss: 2.3590617418289184\n",
            "epoch 162, loss: 2.3614890813827514\n",
            "epoch 163, loss: 2.354063618183136\n",
            "epoch 164, loss: 2.363424301147461\n",
            "epoch 165, loss: 2.3577054381370544\n",
            "epoch 166, loss: 2.351191747188568\n",
            "epoch 167, loss: 2.3548559665679933\n",
            "epoch 168, loss: 2.3515581369400023\n",
            "epoch 169, loss: 2.3580605745315553\n",
            "epoch 170, loss: 2.359258306026459\n",
            "epoch 171, loss: 2.3580939769744873\n",
            "epoch 172, loss: 2.363922488689423\n",
            "epoch 173, loss: 2.3622729182243347\n",
            "epoch 174, loss: 2.3711732983589173\n",
            "epoch 175, loss: 2.3792161107063294\n",
            "epoch 176, loss: 2.39123512506485\n",
            "epoch 177, loss: 2.3840383768081663\n",
            "epoch 178, loss: 2.3807212114334106\n",
            "epoch 179, loss: 2.4113155364990235\n",
            "epoch 180, loss: 2.427864360809326\n",
            "epoch 181, loss: 2.401677691936493\n",
            "epoch 182, loss: 2.379083573818207\n",
            "epoch 183, loss: 2.3678740501403808\n",
            "epoch 184, loss: 2.3584201097488404\n",
            "epoch 185, loss: 2.3464757680892943\n",
            "epoch 186, loss: 2.341503369808197\n",
            "epoch 187, loss: 2.3407013416290283\n",
            "epoch 188, loss: 2.340752923488617\n",
            "epoch 189, loss: 2.343449890613556\n",
            "epoch 190, loss: 2.347569000720978\n",
            "epoch 191, loss: 2.345993733406067\n",
            "epoch 192, loss: 2.347149908542633\n",
            "epoch 193, loss: 2.3455200672149656\n",
            "epoch 194, loss: 2.3480556488037108\n",
            "epoch 195, loss: 2.3475006461143493\n",
            "epoch 196, loss: 2.345742666721344\n",
            "epoch 197, loss: 2.3430111527442934\n",
            "epoch 198, loss: 2.336551010608673\n",
            "epoch 199, loss: 2.333657693862915\n",
            "epoch 200, loss: 2.3331082344055174\n",
            "epoch 201, loss: 2.3340760827064515\n",
            "epoch 202, loss: 2.332259202003479\n",
            "epoch 203, loss: 2.3319934487342833\n",
            "epoch 204, loss: 2.333864438533783\n",
            "epoch 205, loss: 2.332706117630005\n",
            "epoch 206, loss: 2.348957860469818\n",
            "epoch 207, loss: 2.3517248749732973\n",
            "epoch 208, loss: 2.341375494003296\n",
            "epoch 209, loss: 2.348954701423645\n",
            "epoch 210, loss: 2.341511869430542\n",
            "epoch 211, loss: 2.3613523602485658\n",
            "epoch 212, loss: 2.3428903937339784\n",
            "epoch 213, loss: 2.355028438568115\n",
            "epoch 214, loss: 2.3434557557106017\n",
            "epoch 215, loss: 2.3609958410263063\n",
            "epoch 216, loss: 2.3459770917892455\n",
            "epoch 217, loss: 2.3603221774101257\n",
            "epoch 218, loss: 2.355089509487152\n",
            "epoch 219, loss: 2.359883177280426\n",
            "epoch 220, loss: 2.3504894018173217\n",
            "epoch 221, loss: 2.352573311328888\n",
            "epoch 222, loss: 2.3446635365486146\n",
            "epoch 223, loss: 2.346121835708618\n",
            "epoch 224, loss: 2.3377132058143615\n",
            "epoch 225, loss: 2.342430055141449\n",
            "epoch 226, loss: 2.3329573392868044\n",
            "epoch 227, loss: 2.3460731625556948\n",
            "epoch 228, loss: 2.3332534193992616\n",
            "epoch 229, loss: 2.34248366355896\n",
            "epoch 230, loss: 2.336852860450745\n",
            "epoch 231, loss: 2.3488538026809693\n",
            "epoch 232, loss: 2.3446736335754395\n",
            "epoch 233, loss: 2.354234313964844\n",
            "epoch 234, loss: 2.360635793209076\n",
            "epoch 235, loss: 2.3452617049217226\n",
            "epoch 236, loss: 2.3576886296272277\n",
            "epoch 237, loss: 2.3447861313819884\n",
            "epoch 238, loss: 2.345292639732361\n",
            "epoch 239, loss: 2.34656765460968\n",
            "epoch 240, loss: 2.3500134110450746\n",
            "epoch 241, loss: 2.365654945373535\n",
            "epoch 242, loss: 2.347713315486908\n",
            "epoch 243, loss: 2.3513165593147276\n",
            "epoch 244, loss: 2.3460253834724427\n",
            "epoch 245, loss: 2.338851034641266\n",
            "epoch 246, loss: 2.335010814666748\n",
            "epoch 247, loss: 2.3309460282325745\n",
            "epoch 248, loss: 2.3416822075843813\n",
            "epoch 249, loss: 2.331472671031952\n",
            "epoch 250, loss: 2.337559628486633\n",
            "epoch 251, loss: 2.34495130777359\n",
            "epoch 252, loss: 2.333216977119446\n",
            "epoch 253, loss: 2.3384973764419557\n",
            "epoch 254, loss: 2.331591236591339\n",
            "epoch 255, loss: 2.3413716316223145\n",
            "epoch 256, loss: 2.339828610420227\n",
            "epoch 257, loss: 2.3406851530075072\n",
            "epoch 258, loss: 2.3536062002182008\n",
            "epoch 259, loss: 2.3386911392211913\n",
            "epoch 260, loss: 2.3406801342964174\n",
            "epoch 261, loss: 2.34695942401886\n",
            "epoch 262, loss: 2.3310608744621275\n",
            "epoch 263, loss: 2.3340832948684693\n",
            "epoch 264, loss: 2.335524392127991\n",
            "epoch 265, loss: 2.325897181034088\n",
            "epoch 266, loss: 2.3286942839622498\n",
            "epoch 267, loss: 2.3271108508110045\n",
            "epoch 268, loss: 2.327558922767639\n",
            "epoch 269, loss: 2.3276262879371643\n",
            "epoch 270, loss: 2.324655306339264\n",
            "epoch 271, loss: 2.331964612007141\n",
            "epoch 272, loss: 2.325058174133301\n",
            "epoch 273, loss: 2.3297059297561646\n",
            "epoch 274, loss: 2.334044325351715\n",
            "epoch 275, loss: 2.324089002609253\n",
            "epoch 276, loss: 2.3328224301338194\n",
            "epoch 277, loss: 2.3305508375167845\n",
            "epoch 278, loss: 2.3283352136611937\n",
            "epoch 279, loss: 2.3337127327919007\n",
            "epoch 280, loss: 2.3268916487693785\n",
            "epoch 281, loss: 2.3357024431228637\n",
            "epoch 282, loss: 2.3311421632766725\n",
            "epoch 283, loss: 2.331528127193451\n",
            "epoch 284, loss: 2.341493833065033\n",
            "epoch 285, loss: 2.3265493512153625\n",
            "epoch 286, loss: 2.3319859981536863\n",
            "epoch 287, loss: 2.336806333065033\n",
            "epoch 288, loss: 2.326249599456787\n",
            "epoch 289, loss: 2.330855655670166\n",
            "epoch 290, loss: 2.334578216075897\n",
            "epoch 291, loss: 2.322447896003723\n",
            "epoch 292, loss: 2.322599184513092\n",
            "epoch 293, loss: 2.3228772640228272\n",
            "epoch 294, loss: 2.319672131538391\n",
            "epoch 295, loss: 2.322101414203644\n",
            "epoch 296, loss: 2.3224318742752077\n",
            "epoch 297, loss: 2.3224180221557615\n",
            "epoch 298, loss: 2.3197311401367187\n",
            "epoch 299, loss: 2.3187949299812316\n",
            "epoch 300, loss: 2.3220328092575073\n",
            "epoch 301, loss: 2.3177663922309875\n",
            "epoch 302, loss: 2.320218503475189\n",
            "epoch 303, loss: 2.320257306098938\n",
            "epoch 304, loss: 2.3186665177345276\n",
            "epoch 305, loss: 2.3215617656707765\n",
            "epoch 306, loss: 2.3189493656158446\n",
            "epoch 307, loss: 2.3247034907341004\n",
            "epoch 308, loss: 2.319730246067047\n",
            "epoch 309, loss: 2.322742295265198\n",
            "epoch 310, loss: 2.329410767555237\n",
            "epoch 311, loss: 2.31978303194046\n",
            "epoch 312, loss: 2.324487042427063\n",
            "epoch 313, loss: 2.322883093357086\n",
            "epoch 314, loss: 2.3185706615447996\n",
            "epoch 315, loss: 2.3220592379570006\n",
            "epoch 316, loss: 2.320109188556671\n",
            "epoch 317, loss: 2.3257060647010803\n",
            "epoch 318, loss: 2.321298563480377\n",
            "epoch 319, loss: 2.3215208411216737\n",
            "epoch 320, loss: 2.3296175718307497\n",
            "epoch 321, loss: 2.3202056765556334\n",
            "epoch 322, loss: 2.3228352785110475\n",
            "epoch 323, loss: 2.322819972038269\n",
            "epoch 324, loss: 2.3193595886230467\n",
            "epoch 325, loss: 2.322401297092438\n",
            "epoch 326, loss: 2.319794714450836\n",
            "epoch 327, loss: 2.3233364820480347\n",
            "epoch 328, loss: 2.3215784788131715\n",
            "epoch 329, loss: 2.318126749992371\n",
            "epoch 330, loss: 2.3279306173324583\n",
            "epoch 331, loss: 2.3205215096473695\n",
            "epoch 332, loss: 2.321518325805664\n",
            "epoch 333, loss: 2.326468288898468\n",
            "epoch 334, loss: 2.31864755153656\n",
            "epoch 335, loss: 2.3234280347824097\n",
            "epoch 336, loss: 2.323383152484894\n",
            "epoch 337, loss: 2.317012441158295\n",
            "epoch 338, loss: 2.3182607531547545\n",
            "epoch 339, loss: 2.3175171852111816\n",
            "epoch 340, loss: 2.320048427581787\n",
            "epoch 341, loss: 2.3195098996162415\n",
            "epoch 342, loss: 2.318371856212616\n",
            "epoch 343, loss: 2.3237691640853884\n",
            "epoch 344, loss: 2.3175360321998597\n",
            "epoch 345, loss: 2.3202167630195616\n",
            "epoch 346, loss: 2.3252131938934326\n",
            "epoch 347, loss: 2.3166193842887877\n",
            "epoch 348, loss: 2.3189677238464355\n",
            "epoch 349, loss: 2.3198761105537415\n",
            "epoch 350, loss: 2.314994728565216\n",
            "epoch 351, loss: 2.3190309166908265\n",
            "epoch 352, loss: 2.3172998785972596\n",
            "epoch 353, loss: 2.31807519197464\n",
            "epoch 354, loss: 2.3184901237487794\n",
            "epoch 355, loss: 2.3184235453605653\n",
            "epoch 356, loss: 2.32781378030777\n",
            "epoch 357, loss: 2.320902407169342\n",
            "epoch 358, loss: 2.3210147976875306\n",
            "epoch 359, loss: 2.3292045116424562\n",
            "epoch 360, loss: 2.3205018520355223\n",
            "epoch 361, loss: 2.3193371295928955\n",
            "epoch 362, loss: 2.3235587000846865\n",
            "epoch 363, loss: 2.316972029209137\n",
            "epoch 364, loss: 2.316342902183533\n",
            "epoch 365, loss: 2.3184150218963624\n",
            "epoch 366, loss: 2.3150222659111024\n",
            "epoch 367, loss: 2.3147003293037414\n",
            "epoch 368, loss: 2.313460981845856\n",
            "epoch 369, loss: 2.313779127597809\n",
            "epoch 370, loss: 2.3141341686248778\n",
            "epoch 371, loss: 2.312331759929657\n",
            "epoch 372, loss: 2.3155219674110414\n",
            "epoch 373, loss: 2.3132028818130492\n",
            "epoch 374, loss: 2.3130862951278686\n",
            "epoch 375, loss: 2.317086374759674\n",
            "epoch 376, loss: 2.3124164938926697\n",
            "epoch 377, loss: 2.3144055008888245\n",
            "epoch 378, loss: 2.314013159275055\n",
            "epoch 379, loss: 2.312498903274536\n",
            "epoch 380, loss: 2.3158069849014282\n",
            "epoch 381, loss: 2.313570237159729\n",
            "epoch 382, loss: 2.316223347187042\n",
            "epoch 383, loss: 2.3167914152145386\n",
            "epoch 384, loss: 2.3147462010383606\n",
            "epoch 385, loss: 2.321623480319977\n",
            "epoch 386, loss: 2.316594660282135\n",
            "epoch 387, loss: 2.3145575165748595\n",
            "epoch 388, loss: 2.3192300319671633\n",
            "epoch 389, loss: 2.3142852425575255\n",
            "epoch 390, loss: 2.314933347702026\n",
            "epoch 391, loss: 2.31811466217041\n",
            "epoch 392, loss: 2.313024032115936\n",
            "epoch 393, loss: 2.313863158226013\n",
            "epoch 394, loss: 2.314450407028198\n",
            "epoch 395, loss: 2.312664973735809\n",
            "epoch 396, loss: 2.3126157999038695\n",
            "epoch 397, loss: 2.3116668105125426\n",
            "epoch 398, loss: 2.3135964393615724\n",
            "epoch 399, loss: 2.3120713591575623\n",
            "epoch 400, loss: 2.3114305257797243\n",
            "epoch 401, loss: 2.3148399233818053\n",
            "epoch 402, loss: 2.311273121833801\n",
            "epoch 403, loss: 2.312440288066864\n",
            "epoch 404, loss: 2.313433575630188\n",
            "epoch 405, loss: 2.310156261920929\n",
            "epoch 406, loss: 2.3113494634628298\n",
            "epoch 407, loss: 2.3104904770851133\n",
            "epoch 408, loss: 2.312125015258789\n",
            "epoch 409, loss: 2.3124876379966737\n",
            "epoch 410, loss: 2.3134516000747682\n",
            "epoch 411, loss: 2.318250060081482\n",
            "epoch 412, loss: 2.312341332435608\n",
            "epoch 413, loss: 2.314435577392578\n",
            "epoch 414, loss: 2.3158847808837892\n",
            "epoch 415, loss: 2.311452794075012\n",
            "epoch 416, loss: 2.3148357391357424\n",
            "epoch 417, loss: 2.3137129306793214\n",
            "epoch 418, loss: 2.3125328540802004\n",
            "epoch 419, loss: 2.313629674911499\n",
            "epoch 420, loss: 2.312708485126495\n",
            "epoch 421, loss: 2.317577135562897\n",
            "epoch 422, loss: 2.3135687112808228\n",
            "epoch 423, loss: 2.3130292892456055\n",
            "epoch 424, loss: 2.314906370639801\n",
            "epoch 425, loss: 2.310363209247589\n",
            "epoch 426, loss: 2.3127732276916504\n",
            "epoch 427, loss: 2.3125107526779174\n",
            "epoch 428, loss: 2.3105485796928407\n",
            "epoch 429, loss: 2.314535391330719\n",
            "epoch 430, loss: 2.3123780131340026\n",
            "epoch 431, loss: 2.3159594178199767\n",
            "epoch 432, loss: 2.3138806104660032\n",
            "epoch 433, loss: 2.3132293581962586\n",
            "epoch 434, loss: 2.318942677974701\n",
            "epoch 435, loss: 2.313108801841736\n",
            "epoch 436, loss: 2.3120386600494385\n",
            "epoch 437, loss: 2.3140462398529054\n",
            "epoch 438, loss: 2.310251975059509\n",
            "epoch 439, loss: 2.3117943167686463\n",
            "epoch 440, loss: 2.3116269588470457\n",
            "epoch 441, loss: 2.3115555882453918\n",
            "epoch 442, loss: 2.3133958339691163\n",
            "epoch 443, loss: 2.3106870532035826\n",
            "epoch 444, loss: 2.3140978813171387\n",
            "epoch 445, loss: 2.312925136089325\n",
            "epoch 446, loss: 2.3122098088264464\n",
            "epoch 447, loss: 2.3165870308876038\n",
            "epoch 448, loss: 2.311553347110748\n",
            "epoch 449, loss: 2.3117151618003846\n",
            "epoch 450, loss: 2.3131515979766846\n",
            "epoch 451, loss: 2.309790301322937\n",
            "epoch 452, loss: 2.3119962573051454\n",
            "epoch 453, loss: 2.311958932876587\n",
            "epoch 454, loss: 2.3123919129371644\n",
            "epoch 455, loss: 2.310495412349701\n",
            "epoch 456, loss: 2.3117079854011537\n",
            "epoch 457, loss: 2.315861201286316\n",
            "epoch 458, loss: 2.3108837485313414\n",
            "epoch 459, loss: 2.3118475794792177\n",
            "epoch 460, loss: 2.312955343723297\n",
            "epoch 461, loss: 2.309553122520447\n",
            "epoch 462, loss: 2.310638165473938\n",
            "epoch 463, loss: 2.3100226283073426\n",
            "epoch 464, loss: 2.3098950028419494\n",
            "epoch 465, loss: 2.3111940145492555\n",
            "epoch 466, loss: 2.312285280227661\n",
            "epoch 467, loss: 2.3165521740913393\n",
            "epoch 468, loss: 2.311981987953186\n",
            "epoch 469, loss: 2.3107385993003846\n",
            "epoch 470, loss: 2.3138413786888123\n",
            "epoch 471, loss: 2.310247004032135\n",
            "epoch 472, loss: 2.310323143005371\n",
            "epoch 473, loss: 2.309732937812805\n",
            "epoch 474, loss: 2.3076245188713074\n",
            "epoch 475, loss: 2.309340465068817\n",
            "epoch 476, loss: 2.30814266204834\n",
            "epoch 477, loss: 2.307772493362427\n",
            "epoch 478, loss: 2.308934271335602\n",
            "epoch 479, loss: 2.3074700355529787\n",
            "epoch 480, loss: 2.3112062096595762\n",
            "epoch 481, loss: 2.30854070186615\n",
            "epoch 482, loss: 2.3111557602882384\n",
            "epoch 483, loss: 2.3152915835380554\n",
            "epoch 484, loss: 2.3089290618896485\n",
            "epoch 485, loss: 2.3104939818382264\n",
            "epoch 486, loss: 2.3111491799354553\n",
            "epoch 487, loss: 2.3081188440322875\n",
            "epoch 488, loss: 2.311662662029266\n",
            "epoch 489, loss: 2.3107877373695374\n",
            "epoch 490, loss: 2.3090426683425904\n",
            "epoch 491, loss: 2.3098555207252502\n",
            "epoch 492, loss: 2.3082510948181154\n",
            "epoch 493, loss: 2.3105726718902586\n",
            "epoch 494, loss: 2.3089820981025695\n",
            "epoch 495, loss: 2.3086989402770994\n",
            "epoch 496, loss: 2.3119876980781555\n",
            "epoch 497, loss: 2.3084721207618712\n",
            "epoch 498, loss: 2.31112140417099\n",
            "epoch 499, loss: 2.3122573256492616\n",
            "epoch 500, loss: 2.307995593547821\n",
            "Saved 1095 node embeddings (combined) to 'embeddings_fairness_mul.pkl'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "import torch.optim as optim\n",
        "#from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.loader import ClusterData, ClusterLoader\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.nn import PNAConv\n",
        "#import input_graph_creation\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#import input_small_graph_creation\n",
        "from pytorch_metric_learning.losses import NTXentLoss\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "import gc\n",
        "import psutil\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import k_hop_subgraph\n",
        "# Print memory usage\n",
        "print(psutil.virtual_memory())\n",
        "print(torch.__version__)\n",
        "print(torch_geometric.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import PNAConv\n",
        "\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,perceptron_hidden_dim,degree,edge_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = PNAConv(in_channels, hidden_channels,[\"sum\",\"mean\",\"min\",\"max\",\"var\",\"std\"],[\"identity\",\"amplification\",\"attenuation\",\"linear\",\"inverse_linear\"],degree,edge_dim)\n",
        "        self.conv2 = PNAConv(hidden_channels, out_channels,[\"sum\",\"mean\",\"min\",\"max\",\"var\",\"std\"],[\"identity\",\"amplification\",\"attenuation\",\"linear\",\"inverse_linear\"],degree,edge_dim)\n",
        "        #self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        #self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.fc1 = torch.nn.Linear(out_channels, perceptron_hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(perceptron_hidden_dim, out_channels)\n",
        "\n",
        "    def forward(self,data1st_graph,data2nd_graph):\n",
        "\n",
        "        x, edge_index,edge_attr = data1st_graph.x, data1st_graph.edge_index,data1st_graph.edge_attr\n",
        "\n",
        "        x = self.conv1(x, edge_index,edge_attr)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index,edge_attr)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x_graph1 = self.fc2(x)\n",
        "\n",
        "        x, edge_index,edge_attr = data2nd_graph.x, data2nd_graph.edge_index,data2nd_graph.edge_attr\n",
        "        x = self.conv1(x, edge_index,edge_attr)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index,edge_attr)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x_graph2 = self.fc2(x)\n",
        "        return x_graph1,x_graph2\n",
        "# loss_func = NTXentLoss(temperature=0.10)\n",
        "\n",
        "\n",
        "def fairness_similarity_loss(embeddings, fairness_scores, multi_hop_followers, node_id_tensor):\n",
        "    loss = 0.0\n",
        "    count = 0\n",
        "    node_id_to_index = {int(nid): idx for idx, nid in enumerate(node_id_tensor.tolist())}\n",
        "\n",
        "    for infl_id, followers in multi_hop_followers.items():\n",
        "        if infl_id not in fairness_scores or infl_id not in node_id_to_index:\n",
        "            continue\n",
        "\n",
        "        infl_index = node_id_to_index[infl_id]\n",
        "        infl_embed = embeddings[infl_index]\n",
        "        target = fairness_scores[infl_id]\n",
        "\n",
        "        for f_id in followers:\n",
        "            if f_id not in node_id_to_index:\n",
        "                continue\n",
        "            f_index = node_id_to_index[f_id]\n",
        "            sim = F.cosine_similarity(infl_embed.unsqueeze(0), embeddings[f_index].unsqueeze(0))\n",
        "            loss += (sim - target).pow(2)\n",
        "            count += 1\n",
        "\n",
        "    return loss / (count + 1e-8)\n",
        "\n",
        "\n",
        "def nt_xent_loss(z1, z2, fairness_weights, temperature=0.1):\n",
        "    N = min(z1.size(0), z2.size(0))\n",
        "    z1, z2 = z1[:N], z2[:N]\n",
        "    f = fairness_weights[:N]\n",
        "\n",
        "    z = torch.cat([z1, z2], dim=0)  # [2N, D]\n",
        "    z = F.normalize(z, dim=1)\n",
        "    sim_matrix = torch.mm(z, z.t()) / temperature  # [2N, 2N]\n",
        "\n",
        "    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)\n",
        "\n",
        "    loss = 0.0\n",
        "    for idx in range(N):\n",
        "        # Positive similarity: between z1[idx] and z2[idx]\n",
        "        pos_sim = torch.exp(F.cosine_similarity(z1[idx].unsqueeze(0), z2[idx].unsqueeze(0)) / temperature)\n",
        "\n",
        "        # All similarities for z1[idx] and z2[idx] to others (excluding self)\n",
        "        all_sims_1 = torch.exp(sim_matrix[idx, :])\n",
        "        all_sims_2 = torch.exp(sim_matrix[idx + N, :])\n",
        "        all_sims = all_sims_1 + all_sims_2\n",
        "        all_sims = all_sims[~mask[idx]]  # Remove self-similarity\n",
        "        denom = all_sims.sum() + 1e-8\n",
        "\n",
        "        node_loss = -torch.log(pos_sim / denom)\n",
        "        weight = 1\n",
        "        loss += (weight) * node_loss\n",
        "\n",
        "    return loss / N\n",
        "\n",
        "def train(train_loader, train_loader2, multi_hop_followers, fairness_scores, λ=1.0):\n",
        "    model.train()\n",
        "    total_loss_list = []\n",
        "\n",
        "    for data1st_graph, data2nd_graph in zip(train_loader, train_loader2):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        h_1, h_2 = model(data1st_graph, data2nd_graph)\n",
        "\n",
        "        # Match sizes if needed\n",
        "        if h_2.shape[0] > h_1.shape[0]:\n",
        "            h_2 = h_2[:h_1.shape[0]]\n",
        "        else:\n",
        "            h_1 = h_1[:h_2.shape[0]]\n",
        "\n",
        "        fairness_weights = data1st_graph.fairness[:h_1.shape[0]].to(h_1.device)\n",
        "\n",
        "        # Compute NT-Xent Loss\n",
        "        loss_ntx = nt_xent_loss(h_1, h_2, fairness_weights, temperature=0.1)\n",
        "\n",
        "        # Compute Fairness Similarity Loss\n",
        "        concat_embeddings = torch.cat([h_1, h_2], dim=0)\n",
        "        loss_fair_sim = fairness_similarity_loss(\n",
        "            concat_embeddings,\n",
        "            fairness_scores,\n",
        "            multi_hop_followers,\n",
        "            data1st_graph.node_id[:concat_embeddings.size(0)]\n",
        "        )\n",
        "\n",
        "        total_loss = loss_ntx + λ * loss_fair_sim\n",
        "\n",
        "        # Step 1: backprop on tensor\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step 2: log float version of loss\n",
        "        total_loss_list.append(total_loss.item())\n",
        "\n",
        "    return sum(total_loss_list) / len(total_loss_list)\n",
        "\n",
        "def save_embeddings(model, loader1, loader2, output_path=\"embeddings_fairness_mul.pkl\"):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_node_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data1, data2 in zip(loader1, loader2):\n",
        "            # Move to device\n",
        "            data1 = data1.to(next(model.parameters()).device)\n",
        "            data2 = data2.to(next(model.parameters()).device)\n",
        "\n",
        "            h1, h2 = model(data1, data2)\n",
        "\n",
        "            # Truncate both embeddings and node_id to the same minimum length\n",
        "            min_len = min(h1.size(0), h2.size(0), data1.node_id.size(0))\n",
        "            h1 = h1[:min_len]\n",
        "            h2 = h2[:min_len]\n",
        "            node_ids = data1.node_id[:min_len]\n",
        "\n",
        "            # Concatenate embeddings from both graphs\n",
        "            combined_embedding = torch.cat([h1, h2], dim=1)\n",
        "\n",
        "            all_embeddings.append(combined_embedding.cpu())\n",
        "            all_node_ids.extend(node_ids.cpu().numpy())\n",
        "\n",
        "    final_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
        "\n",
        "    embeddings_dict = {\n",
        "        \"node_ids\": all_node_ids,\n",
        "        \"embeddings\": final_embeddings,\n",
        "    }\n",
        "\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        pickle.dump(embeddings_dict, f)\n",
        "\n",
        "    print(f\"Saved {len(all_node_ids)} node embeddings (combined) to '{output_path}'\")\n",
        "\n",
        "\n",
        "def manual_cluster_split(input_graph1, input_graph2, num_parts=10):\n",
        "    assert input_graph1.num_nodes == input_graph2.num_nodes, \"Node count mismatch!\"\n",
        "    num_nodes = input_graph1.num_nodes\n",
        "    # node_indices = torch.randperm(num_nodes)\n",
        "    node_indices = torch.arange(num_nodes)\n",
        "    clusters_1 = []\n",
        "    clusters_2 = []\n",
        "\n",
        "    part_size = (num_nodes + num_parts - 1) // num_parts  # ceiling division\n",
        "    for i in range(0, num_nodes, part_size):\n",
        "      idx = node_indices[i:i+part_size]\n",
        "\n",
        "      sub1 = input_graph1.subgraph(idx)\n",
        "      sub2 = input_graph2.subgraph(idx)\n",
        "\n",
        "      sub1.node_id = input_graph1.node_id[idx]\n",
        "      sub2.node_id = input_graph2.node_id[idx]\n",
        "\n",
        "      sub1.fairness = input_graph1.fairness[idx]\n",
        "      sub2.fairness = input_graph2.fairness[idx]\n",
        "\n",
        "      clusters_1.append(sub1)\n",
        "      clusters_2.append(sub2)\n",
        "\n",
        "    return clusters_1, clusters_2\n",
        "\n",
        "def loader_func(input_graph1, input_graph2):\n",
        "    # Get matching clusters from both graphs\n",
        "    cluster_subgraphs_1, cluster_subgraphs_2 = manual_cluster_split(input_graph1, input_graph2, num_parts=10)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader_1 = DataLoader(cluster_subgraphs_1, batch_size=1, shuffle=False)\n",
        "    train_loader_2 = DataLoader(cluster_subgraphs_2, batch_size=1, shuffle=False)\n",
        "\n",
        "    return train_loader_1, train_loader_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "in_channels=100\n",
        "hidden_channels = 80  # Example value, you can change it\n",
        "#out_channels = 120  # Example value, you can change it\n",
        "out_channels = 80\n",
        "#perceptron_hidden_dim = 250\n",
        "perceptron_hidden_dim = 80\n",
        "degree=torch.tensor([240, 328,  79,  39,  23,  12,  11,   7,   6,   5,   7,   3,   1,   0,\n",
        "          2,   0,   0,   0,   1])\n",
        "\n",
        "edge_dim=100\n",
        "model = Net(in_channels, hidden_channels, out_channels,perceptron_hidden_dim,degree,edge_dim)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "with open(\"multi_hop_followers.pkl\", \"rb\") as f:\n",
        "    multi_hop_followers = pickle.load(f)\n",
        "\n",
        "with open(\"node_fairness_scores.pkl\", \"rb\") as f:\n",
        "    fairness_scores = pickle.load(f)\n",
        "\n",
        "graph_list=[]\n",
        "times=1\n",
        "while(times<3):\n",
        "    # random_choice = times\n",
        "    random_choice = np.random.choice([1, 2])\n",
        "    data_graph=func_graph_augmentation(random_choice)\n",
        "    graph_list.append(data_graph)\n",
        "    times=times+1\n",
        "train_loader,train_loader2=loader_func(graph_list[0],graph_list[1])\n",
        "for epoch in range(1, 501):\n",
        "\n",
        "    loss = train(train_loader,train_loader2,multi_hop_followers,fairness_scores)\n",
        "    print(f\"epoch {epoch}, loss: {loss}\")\n",
        "\n",
        "save_embeddings(model, train_loader, train_loader2, output_path=\"embeddings_fairness_mul.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sZb63tQcgd9",
        "outputId": "ce95c079-25c7-4d78-9bce-f1cd9ca4b368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in pickle file: dict_keys(['node_ids', 'embeddings'])\n",
            " embeddings: [ 6.17513992e-02 -1.95442036e-01 -4.33062240e-02 -4.52255517e-01\n",
            " -6.65501505e-02  1.59940310e-02 -1.10790275e-01 -1.46443799e-01\n",
            " -1.68256462e-01 -1.99117698e-04  5.69934025e-04  9.11595076e-02\n",
            " -1.15890771e-01 -9.44496170e-02 -2.74717808e-01  2.18510211e-01\n",
            " -6.13685064e-02 -5.07791489e-02  7.39697739e-02  3.30987573e-03\n",
            "  1.26159191e-01  9.74479020e-02 -1.23606429e-01 -2.18232274e-01\n",
            " -8.06887001e-02 -1.21948987e-01  1.15545109e-01 -1.51796833e-01\n",
            "  1.62182719e-01 -1.56608984e-01  1.03809595e-01  4.84202281e-02\n",
            " -1.66529492e-01 -1.81111738e-01 -5.73868514e-04  1.25083596e-01\n",
            "  1.17561162e-01  2.10701972e-01 -1.38994232e-01 -6.49110274e-03\n",
            " -6.34399951e-02  7.06671085e-03  2.07076624e-01  5.68343364e-02\n",
            " -1.61285415e-01 -1.03544429e-01  1.37668416e-01  2.05318704e-02\n",
            " -1.81509212e-01 -7.10483044e-02 -5.15347123e-02 -6.06839061e-02\n",
            "  2.58787349e-03 -1.29528105e-01  1.85995713e-01  1.03251003e-01\n",
            "  1.44273430e-01 -1.83269024e-01 -1.45684361e-01 -7.33095631e-02\n",
            " -6.80904537e-02  1.23031445e-01  5.91339022e-02 -3.91790876e-03\n",
            " -7.81970993e-02 -4.90984246e-02  6.18571695e-03  2.56223500e-01\n",
            "  3.13371718e-01 -1.68618225e-02 -8.19849595e-03 -1.03740677e-01\n",
            "  5.60550578e-02  3.55258286e-02  2.46653885e-01  1.39076486e-02\n",
            " -1.34120867e-01  9.58517715e-02  1.21834129e-01  9.54462290e-02\n",
            "  6.17513992e-02 -1.95442036e-01 -4.33062240e-02 -4.52255517e-01\n",
            " -6.65501505e-02  1.59940310e-02 -1.10790275e-01 -1.46443799e-01\n",
            " -1.68256462e-01 -1.99117698e-04  5.69934025e-04  9.11595076e-02\n",
            " -1.15890771e-01 -9.44496170e-02 -2.74717808e-01  2.18510211e-01\n",
            " -6.13685064e-02 -5.07791489e-02  7.39697739e-02  3.30987573e-03\n",
            "  1.26159191e-01  9.74479020e-02 -1.23606429e-01 -2.18232274e-01\n",
            " -8.06887001e-02 -1.21948987e-01  1.15545109e-01 -1.51796833e-01\n",
            "  1.62182719e-01 -1.56608984e-01  1.03809595e-01  4.84202281e-02\n",
            " -1.66529492e-01 -1.81111738e-01 -5.73868514e-04  1.25083596e-01\n",
            "  1.17561162e-01  2.10701972e-01 -1.38994232e-01 -6.49110274e-03\n",
            " -6.34399951e-02  7.06671085e-03  2.07076624e-01  5.68343364e-02\n",
            " -1.61285415e-01 -1.03544429e-01  1.37668416e-01  2.05318704e-02\n",
            " -1.81509212e-01 -7.10483044e-02 -5.15347123e-02 -6.06839061e-02\n",
            "  2.58787349e-03 -1.29528105e-01  1.85995713e-01  1.03251003e-01\n",
            "  1.44273430e-01 -1.83269024e-01 -1.45684361e-01 -7.33095631e-02\n",
            " -6.80904537e-02  1.23031445e-01  5.91339022e-02 -3.91790876e-03\n",
            " -7.81970993e-02 -4.90984246e-02  6.18571695e-03  2.56223500e-01\n",
            "  3.13371718e-01 -1.68618225e-02 -8.19849595e-03 -1.03740677e-01\n",
            "  5.60550578e-02  3.55258286e-02  2.46653885e-01  1.39076486e-02\n",
            " -1.34120867e-01  9.58517715e-02  1.21834129e-01  9.54462290e-02]\n",
            " embeddings: [-0.01471914 -0.05529793 -0.00459805 -0.07377754  0.09244809 -0.0016233\n",
            " -0.02998929  0.08131594 -0.053673    0.01567957 -0.00292679  0.00236482\n",
            " -0.03538951 -0.05523556  0.08617011  0.03629676 -0.06908407 -0.08891217\n",
            "  0.08713122 -0.0035636  -0.0207371  -0.05349633  0.00319934  0.08933939\n",
            "  0.00085796  0.09204999 -0.11568835  0.07037394  0.00680149  0.04456693\n",
            "  0.03280291 -0.01579011 -0.09896076  0.10039103 -0.00269127 -0.05312177\n",
            "  0.02189574  0.04502088  0.0952435   0.10527234  0.00567726 -0.01297735\n",
            " -0.17079869 -0.00606206  0.09025227 -0.03479483 -0.08781503 -0.10199402\n",
            "  0.04098447  0.02266937  0.09889976 -0.05672079  0.01443466  0.10361525\n",
            "  0.01046077  0.00349877  0.05292638 -0.04347907  0.01347192  0.01952001\n",
            "  0.04245141  0.05295547  0.06050625 -0.05433077  0.0171028   0.07617842\n",
            " -0.02736285 -0.07631376 -0.07169051 -0.08278696 -0.06049012 -0.08859309\n",
            " -0.00955291 -0.07156791 -0.06829162  0.00218187  0.04149907 -0.02563466\n",
            "  0.10811248 -0.12829559 -0.01471914 -0.05529793 -0.00459805 -0.07377754\n",
            "  0.09244809 -0.0016233  -0.02998929  0.08131594 -0.053673    0.01567957\n",
            " -0.00292679  0.00236482 -0.03538951 -0.05523556  0.08617011  0.03629676\n",
            " -0.06908407 -0.08891217  0.08713122 -0.0035636  -0.0207371  -0.05349633\n",
            "  0.00319934  0.08933939  0.00085796  0.09204999 -0.11568835  0.07037394\n",
            "  0.00680149  0.04456693  0.03280291 -0.01579011 -0.09896076  0.10039103\n",
            " -0.00269127 -0.05312177  0.02189574  0.04502088  0.0952435   0.10527234\n",
            "  0.00567726 -0.01297735 -0.17079869 -0.00606206  0.09025227 -0.03479483\n",
            " -0.08781503 -0.10199402  0.04098447  0.02266937  0.09889976 -0.05672079\n",
            "  0.01443466  0.10361525  0.01046077  0.00349877  0.05292638 -0.04347907\n",
            "  0.01347192  0.01952001  0.04245141  0.05295547  0.06050625 -0.05433077\n",
            "  0.0171028   0.07617842 -0.02736285 -0.07631376 -0.07169051 -0.08278696\n",
            " -0.06049012 -0.08859309 -0.00955291 -0.07156791 -0.06829162  0.00218187\n",
            "  0.04149907 -0.02563466  0.10811248 -0.12829559]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickle file\n",
        "with open(\"embeddings_fairness_mul.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Print keys\n",
        "print(\"Keys in pickle file:\", data.keys())\n",
        "for i in range(2):\n",
        "    print(f\" embeddings: {data['embeddings'][i]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
